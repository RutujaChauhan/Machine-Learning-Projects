{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PBrd0kpkPR9n"
   },
   "source": [
    "# Case Study 1: New York City Taxi Fare Prediction\n",
    "\n",
    "\n",
    "![](nyc_taxi_banner.jpg)\n",
    "\n",
    "\n",
    "The dataset here consists of historical data pertaining to fare amounts (inclusive of tolls) for taxi rides in New York City. There are various attributes here including the fare amount, pickup times, pickup and dropoff co-ordinates and the passenger count! \n",
    "\n",
    "The key idea here is if we can build a model on this datset to predict the potential taxi fare for a future taxi ride in NYC given that we know the other attibutes except fare\n",
    "\n",
    "__Main Objective:__ Given a ride's pickup time, pickup and dropoff coordinates along with the total passengers riding, build a model to predict the fare for a NYC taxi (regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fkyNJbuXPR9p"
   },
   "source": [
    "# Load up basic dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XbVeNDewPR9q"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1twmS94APR9t"
   },
   "source": [
    "#### Since this is a regression problem, we will use R-square, Adjusted R-square and RMSE to evaluate our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YWASIMHrPR9u"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "def adjusted_r2_score(y_true, y_pred, X_test):\n",
    "    r2 = r2_score(y_true=y_true, y_pred=y_pred)\n",
    "    adjusted_r2 = 1 - (1-r2)*(len(y_true)-1)/(len(y_true) - X_test.shape[1]-1)\n",
    "    return adjusted_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "77DIoggqPR9w"
   },
   "source": [
    "# Load and View the Dataset\n",
    "\n",
    "There are over __50 million__ datapoints in this dataset! We load around __10 million__ datapoints for this case study \n",
    "\n",
    "The data is available at https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/data from where you can download it.\n",
    "\n",
    "We recommend using the kaggle API and the following command via CLI to get it.\n",
    "\n",
    "__`kaggle competitions download -c new-york-city-taxi-fare-prediction`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Cz22rhgPR9x",
    "outputId": "6a402a7e-2c01-4121-b389-d1f2217bbb99"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:586\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    571\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    572\u001b[0m     dialect,\n\u001b[0;32m    573\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    583\u001b[0m )\n\u001b[0;32m    584\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:482\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    479\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:811\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwds:\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 811\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1040\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1037\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown engine: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (valid options are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1038\u001b[0m     )\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;66;03m# error: Too many arguments for \"ParserBase\"\u001b[39;00m\n\u001b[1;32m-> 1040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:51\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     48\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musecols\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musecols\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# open handles\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open_handles\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Have to pass int, would break tests using TextReader directly otherwise :(\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py:222\u001b[0m, in \u001b[0;36mParserBase._open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_handles\u001b[39m(\u001b[38;5;28mself\u001b[39m, src: FilePathOrBuffer, kwds: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;124;03m    Let the readers open IOHandles after they are done with their potential raises.\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:702\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    698\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    701\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 702\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    710\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    711\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_csv('train.csv', nrows=10000000)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jq4u0by1PR90",
    "outputId": "106b9aca-5a37-4404-9472-4867350151e3"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v3ctroDkPR93"
   },
   "source": [
    "# Basic Data Processing\n",
    "\n",
    "- Removing any rows with null values\n",
    "- Converting the pickup datetime attribute to a datetime type from string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "38eN_TaLPR93"
   },
   "outputs": [],
   "source": [
    "df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'], format='%Y-%m-%d %H:%M:%S UTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I1bMYeAxPR96",
    "outputId": "ae140555-e464-465f-acaf-2ec968769b40"
   },
   "outputs": [],
   "source": [
    "df.dropna(how='any', axis='rows', inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VSQeLAn_PR98"
   },
   "source": [
    "# Build train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QKOEFSGbPR98",
    "outputId": "8d3cff00-2f14-486e-cfcc-5d6e5cd4ffe8"
   },
   "outputs": [],
   "source": [
    "features = df[['pickup_datetime', 'pickup_longitude', 'pickup_latitude', \n",
    "               'dropoff_longitude', 'dropoff_latitude', 'passenger_count']]\n",
    "price = df['fare_amount']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, price, test_size=0.3, random_state=42)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YDgSz1boPR9-"
   },
   "source": [
    "# Experiment 1: Build a Baseline Model with Raw Features\n",
    "\n",
    "No feature engineering, just basic null removal for data wrangling. Let's build a quick baseline model on the raw features and evaluate its performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y9g1rZ7JPR9_",
    "outputId": "2c938e16-5737-428c-8fe1-bd6324404653"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "xgr = xgb.XGBRegressor(objective='reg:linear', n_estimators=50, max_depth=5, n_jobs=-1, random_state=42)\n",
    "xgr.fit(X_train.drop(columns=['pickup_datetime']), y_train)\n",
    "\n",
    "y_pred = xgr.predict(X_test.drop(columns=['pickup_datetime']))\n",
    "\n",
    "rsq_baseline_xgb = r2_score(y_true=y_test, y_pred=y_pred)\n",
    "adj_rsq_baseline_xgb = adjusted_r2_score(y_true=y_test, y_pred=y_pred, X_test=X_test)\n",
    "rmse_baseline_xgb = mean_squared_error(y_true=y_test, y_pred=y_pred) ** 0.5\n",
    "print('R-sq:', rsq_baseline_xgb)\n",
    "print('Adj. R-sq:', adj_rsq_baseline_xgb)\n",
    "print('RMSE:', rmse_baseline_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uzt5MBBkPR-B"
   },
   "source": [
    "We have __R-sq: 0.73__ and __RMSE: 5.095__ in our baseline model. Can we do better than this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EwGHKwfCPR-B"
   },
   "source": [
    "# Exploratory Data Analysis for Outlier Detection\n",
    "\n",
    "The key intent for doing EDA here is to see if we can find any potential latent patterns in our data which can help with either deriving new features or detecting potential anomalies which have adverse effects on machine learning models. We will use the following techniques here:\n",
    "\n",
    "- Descriptive Statistics\n",
    "- Basic Data Visualization\n",
    "- Domain Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9gvlktIPPR-C"
   },
   "source": [
    "### Look at basic descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dANTa9sDPR-D",
    "outputId": "1fa1e12b-4f0d-4c52-c0a8-d78bfa77ca1f"
   },
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-StRKhpwPR-E"
   },
   "source": [
    "### Visualizing Trip Fare Amount\n",
    "\n",
    "Looks like a right skewed distribution with some extreme values of trips with really high fare. Also some negative fare values! Maybe potential outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9OoAWskVPR-F",
    "outputId": "f0ce7724-46c8-47aa-c4d2-f7fbd3093e76"
   },
   "outputs": [],
   "source": [
    "p = sns.kdeplot((df['fare_amount'].values), shade=True)\n",
    "t = plt.suptitle(\"Distribution of fare amount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EUTLObThPR-H"
   },
   "source": [
    "### Visualizing key bounding co-ordinates in the US - Latitude & Longitude\n",
    "\n",
    "The following map gives us a good idea of what should be the ideal bounding box latitude and longitude values for the US\n",
    "\n",
    "\n",
    "![aa](us_lat_long.PNG \"US Map depicting Latitude & Longitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7zzfpxotPR-H"
   },
   "source": [
    "### Visualizing Distribution of Pickup Co-ordinates in the US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_7qlcxWmPR-I",
    "outputId": "f71acdfa-61f5-4cf3-8f78-90492d96e844"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (14, 5))\n",
    "title = fig.suptitle(\"Distribution of trips across the US\", fontsize=14)\n",
    "ax1 = fig.add_subplot(1,2, 1)\n",
    "p = sns.kdeplot((df[(df['pickup_latitude']>= 30) & (df['pickup_latitude'] <= 50)]['pickup_latitude'].values),\n",
    "                shade=True,\n",
    "                ax=ax1)\n",
    "t= ax1.set_title(\"Distribution of latitude\")\n",
    "\n",
    "ax2 = fig.add_subplot(1,2, 2)\n",
    "p = sns.kdeplot((df[(df['pickup_longitude']>= -125) & (df['pickup_longitude'] <= -65)]['pickup_longitude'].values),\n",
    "                shade=True,\n",
    "                ax=ax2)\n",
    "t = ax2.set_title(\"Distribution of longitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mh5yumhxPR-K"
   },
   "source": [
    "### Visualizing distribution of typical passenger counts in trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3GVjQ4EVPR-K",
    "outputId": "1fc57c3d-ff0c-40a2-e205-620e51a2ed7c"
   },
   "outputs": [],
   "source": [
    "p = sns.kdeplot((df['passenger_count'].values), shade=True)\n",
    "t= plt.suptitle(\"Distribution of Passenger Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oa1f4Rm-PR-M"
   },
   "source": [
    "## Fix co-ordinates to be in the US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RdmCJsWtPR-N"
   },
   "outputs": [],
   "source": [
    "lat_long = {\n",
    "    'min_lat':30,\n",
    "    'max_lat':50,    \n",
    "    'min_long':-125,\n",
    "    'max_long':-65, \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t7b5IGc5PR-O"
   },
   "source": [
    "# Data Wrangling - Outlier Removal\n",
    "\n",
    "Based on the knowledge from EDA we can focus on removing the following outliers\n",
    "\n",
    "- Fare amount should be positive and < 1000\n",
    "- A typical taxi or cab can maybe take at the max 8 people and minimum 1 person \n",
    "- Pickup and Dropoff Co-ordinates to be in the US bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pAV5G3nVPR-P",
    "outputId": "e5bc3b0c-b66f-4e60-b70f-d3e0a2a63da5"
   },
   "outputs": [],
   "source": [
    "bad_filter_cond = (df['fare_amount'].between(0.01, 1000) \n",
    "                   & df['passenger_count'].between(1, 8)\n",
    "                   & df['pickup_latitude'].between(lat_long['min_lat'], lat_long['max_lat'])\n",
    "                   & df['dropoff_latitude'].between(lat_long['min_lat'], lat_long['max_lat']) \n",
    "                   & df['pickup_longitude'].between(lat_long['min_long'], lat_long['max_long'])\n",
    "                   & df['dropoff_longitude'].between(lat_long['min_long'], lat_long['max_long']))\n",
    "\n",
    "df = df[bad_filter_cond]\n",
    "\n",
    "features = df[['pickup_datetime', 'pickup_longitude', 'pickup_latitude', \n",
    "               'dropoff_longitude', 'dropoff_latitude', 'passenger_count']]\n",
    "price = df['fare_amount']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, price, test_size=0.3, random_state=42)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HHGVdHJXPR-R",
    "outputId": "bd2ea7ac-4112-4a93-ac00-aba56569009c"
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "swNlOdcMPR-T"
   },
   "source": [
    "# Experiment 2: Baseline Model after Outlier Removal\n",
    "\n",
    "We rebuild our new baseline model with the raw features after removing the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Eop4-kvRPR-V",
    "outputId": "e70d65f4-c92e-423f-be4c-4e61c8a49cca"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "xgr = xgb.XGBRegressor(objective='reg:linear', n_estimators=50, max_depth=5, n_jobs=-1, random_state=42)\n",
    "xgr.fit(X_train.drop(columns=['pickup_datetime']), y_train)\n",
    "\n",
    "y_pred = xgr.predict(X_test.drop(columns=['pickup_datetime']))\n",
    "\n",
    "rsq_baseline2_xgb = r2_score(y_true=y_test, y_pred=y_pred)\n",
    "adj_rsq_baseline2_xgb = adjusted_r2_score(y_true=y_test, y_pred=y_pred, X_test=X_test)\n",
    "rmse_baseline2_xgb = mean_squared_error(y_true=y_test, y_pred=y_pred) ** 0.5\n",
    "print('R-sq:', rsq_baseline2_xgb)\n",
    "print('Adj. R-sq:', adj_rsq_baseline2_xgb)\n",
    "print('RMSE:', rmse_baseline2_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q3xb8wgNPR-Z"
   },
   "source": [
    "We have __R-sq: 0.76__ and __RMSE: 4.738__ in our new baseline model. \n",
    "\n",
    "This is an improvement of __3% R-sq__ and __0.357__ drop in RMSE which is good!\n",
    "\n",
    "Can we do better than this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7ZFvFYjPR-a"
   },
   "source": [
    "# Experiment 3: Temporal Features from Date and Time\n",
    "\n",
    "We will leverage the pickup datetime attribute to extract some temporal features which could be useful\n",
    "\n",
    "- Year\n",
    "- Month\n",
    "- Day\n",
    "- Hour\n",
    "- Day of the Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fxGIZlp5PR-a"
   },
   "outputs": [],
   "source": [
    "X_train['year'] = X_train['pickup_datetime'].dt.year\n",
    "X_train['month'] = X_train['pickup_datetime'].dt.month\n",
    "X_train['day'] = X_train['pickup_datetime'].dt.day\n",
    "X_train['hour'] = X_train['pickup_datetime'].dt.hour\n",
    "X_train['day_of_week'] = X_train['pickup_datetime'].dt.weekday\n",
    "\n",
    "\n",
    "X_test['year'] = X_test['pickup_datetime'].dt.year\n",
    "X_test['month'] = X_test['pickup_datetime'].dt.month\n",
    "X_test['day'] = X_test['pickup_datetime'].dt.day\n",
    "X_test['hour'] = X_test['pickup_datetime'].dt.hour\n",
    "X_test['day_of_week'] = X_test['pickup_datetime'].dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3fs5AJLDPR-c",
    "outputId": "0f7742d5-9419-4841-8241-08399cb9a760"
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Z6hR1S0PR-e",
    "outputId": "894d5c6a-f612-462f-9b5e-4fc111b70eb7"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "xgr = xgb.XGBRegressor(objective='reg:linear', n_estimators=50, max_depth=5, n_jobs=-1, random_state=42)\n",
    "xgr.fit(X_train.drop(columns=['pickup_datetime']), y_train)\n",
    "\n",
    "y_pred = xgr.predict(X_test.drop(columns=['pickup_datetime']))\n",
    "\n",
    "rsq_temporal_xgb = r2_score(y_true=y_test, y_pred=y_pred)\n",
    "adj_rsq_temporal_xgb = adjusted_r2_score(y_true=y_test, y_pred=y_pred, X_test=X_test)\n",
    "rmse_temporal_xgb = mean_squared_error(y_true=y_test, y_pred=y_pred) ** 0.5\n",
    "print('R-sq:', rsq_temporal_xgb)\n",
    "print('Adj. R-sq:', adj_rsq_temporal_xgb)\n",
    "print('RMSE:', rmse_temporal_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GurW29F4PR-f"
   },
   "source": [
    "We have __R-sq: 0.775__ and __RMSE: 4.602__ in our new model. \n",
    "\n",
    "This is an improvement of __1.5% R-sq__ and __0.136__ drop in RMSE which is quite good!\n",
    "\n",
    "Can we do better than this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lLWZgKAkPR-g"
   },
   "source": [
    "# Experiment 4: Adding Trip Distance as a feature - Haversine Distance\n",
    "\n",
    "The Earth is round but big, so we can consider it flat for short distances. However, flat-Earth formulas for calculating the distance between two points start showing noticeable errors when the distance is more than about 20 kilometers\n",
    "\n",
    "Therefore, calculating distances on a sphere needs to consider spherical geometry\n",
    "\n",
    "The haversine formula is a very accurate way of computing distances between two points on the surface of a sphere using the latitude and longitude of the two points\n",
    "\n",
    "![](haversine.PNG)\n",
    "\n",
    "#### Haversine Formula \n",
    "\n",
    "The word \"Haversine\" comes from the function: __haversine(θ) = sin²(θ/2)__\n",
    "\n",
    "We can further derive the following:\n",
    "\n",
    "__a = sin²(φB - φA/2) + cos φA * cos φB * sin²(λB - λA/2)__\n",
    "\n",
    "__c = 2 * atan2( √a, √(1−a) )__\n",
    "\n",
    "__d = R ⋅ c__\n",
    "\n",
    "The following equation where,\n",
    "\n",
    "- __φ is latitude__\n",
    "- __λ is longitude__\n",
    "- __R is earth’s radius__ \n",
    "- __d is the haversine distance__\n",
    "\n",
    "Note that angles need to be in radians to pass to trig functions\n",
    "\n",
    "Source: https://community.esri.com/groups/coordinate-reference-systems/blog/2017/10/05/haversine-formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ciibAMbOPR-g"
   },
   "outputs": [],
   "source": [
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "AVG_EARTH_RADIUS_KM = 6371.0088\n",
    "AVG_EARTH_RADIUS_MI = 3958.7613\n",
    "\n",
    "def haversine(start_coord, end_coord, miles=False):\n",
    "    # get earth radius in required units\n",
    "    if miles:\n",
    "        avg_earth_radius = AVG_EARTH_RADIUS_MI\n",
    "    else:\n",
    "        avg_earth_radius = AVG_EARTH_RADIUS_KM\n",
    "\n",
    "    # unpack latitude/longitude\n",
    "    lat1, lng1 = start_coord\n",
    "    lat2, lng2 = end_coord\n",
    "\n",
    "    # convert all latitudes/longitudes from decimal degrees to radians\n",
    "    lat1, lng1, lat2, lng2 = map(radians, (lat1, lng1, lat2, lng2))\n",
    "\n",
    "    # calculate haversine\n",
    "    lat = lat2 - lat1\n",
    "    lng = lng2 - lng1\n",
    "    d = sin(lat * 0.5) ** 2 + cos(lat1) * cos(lat2) * sin(lng * 0.5) ** 2\n",
    "    \n",
    "    return 2 * avg_earth_radius * asin(sqrt(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qJjZSzA_PR-i"
   },
   "outputs": [],
   "source": [
    "X_train.drop(columns=['pickup_datetime'], inplace=True)\n",
    "X_test.drop(columns=['pickup_datetime'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EgzG5GkAPR-k",
    "outputId": "90ddcd8a-f353-4b8b-bbb0-2ebcf044324c"
   },
   "outputs": [],
   "source": [
    "X_train['haversine_dist'] = X_train.apply(lambda row: haversine(start_coord=(row['pickup_latitude'], \n",
    "                                                                             row['pickup_longitude']),\n",
    "                                                                end_coord=(row['dropoff_latitude'], \n",
    "                                                                           row['dropoff_longitude'])), axis=1)\n",
    "\n",
    "X_test['haversine_dist'] = X_test.apply(lambda row: haversine(start_coord=(row['pickup_latitude'], \n",
    "                                                                             row['pickup_longitude']),\n",
    "                                                                end_coord=(row['dropoff_latitude'], \n",
    "                                                                           row['dropoff_longitude'])), axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XnfQPXKBPR-n",
    "outputId": "f1906df6-efbb-4aae-f6c4-5862a5cc8d77"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "xgr = xgb.XGBRegressor(objective='reg:linear', n_estimators=50, max_depth=5, n_jobs=-1, random_state=42)\n",
    "xgr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgr.predict(X_test)\n",
    "\n",
    "rsq_haversine_xgb = r2_score(y_true=y_test, y_pred=y_pred)\n",
    "adj_rsq_haversine_xgb = adjusted_r2_score(y_true=y_test, y_pred=y_pred, X_test=X_test)\n",
    "rmse_haversine_xgb = mean_squared_error(y_true=y_test, y_pred=y_pred) ** 0.5\n",
    "print('R-sq:', rsq_haversine_xgb)\n",
    "print('Adj. R-sq:', adj_rsq_haversine_xgb)\n",
    "print('RMSE:', rmse_haversine_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wbUZHSlgPR-p"
   },
   "source": [
    "We have __R-sq: 0.83__ and __RMSE: 3.997__ in our new model. \n",
    "\n",
    "This is an improvement of __5.5% R-sq__ and __0.605__ drop in RMSE which is really excellent!\n",
    "\n",
    "Can we do better than this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9hnOxc6kPR-p"
   },
   "source": [
    "# Experiment 5: Experimenting with Manhattan Distance as a feature\n",
    "\n",
    "\n",
    "Based on the gridlike street geography of the New York borough of Manhattan.\n",
    "\n",
    "The distance between two points measured along axes at right angles. \n",
    "\n",
    "![](manhattan.PNG)\n",
    "\n",
    "In taxicab geometry, the red, yellow, and blue paths all have the same shortest path length of 12.\n",
    "\n",
    "In a plane with __$p_1$__ at __($x_1, y_1$)__ and __$p_2$__ at __($x_2, y_2$)__,\n",
    "\n",
    "Manhattan Distance, __M = |$x_1 - x_2$| + |$y_1 - y_2$|__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yp1yxhA8PR-q"
   },
   "outputs": [],
   "source": [
    "def manhattan(start_coord, end_coord):\n",
    "    \n",
    "    pickup_lat, pickup_long = start_coord\n",
    "    dropoff_lat, dropoff_long = end_coord    \n",
    "    distance = np.abs(dropoff_lat - pickup_lat) + np.abs(dropoff_long - pickup_long)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vP6xBPY0PR-s",
    "outputId": "52c2a8ea-6946-4691-dc30-8446daf380fb"
   },
   "outputs": [],
   "source": [
    "X_train['manhattan_dist'] = X_train.apply(lambda row: manhattan(start_coord=(row['pickup_latitude'], \n",
    "                                                                             row['pickup_longitude']),\n",
    "                                                                end_coord=(row['dropoff_latitude'], \n",
    "                                                                           row['dropoff_longitude'])), axis=1)\n",
    "\n",
    "X_test['manhattan_dist'] = X_test.apply(lambda row: manhattan(start_coord=(row['pickup_latitude'], \n",
    "                                                                             row['pickup_longitude']),\n",
    "                                                                end_coord=(row['dropoff_latitude'], \n",
    "                                                                           row['dropoff_longitude'])), axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gN4q-LsRPR-u",
    "outputId": "75f82cfd-eebb-4f8e-d041-b219d1bf89eb"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "xgr = xgb.XGBRegressor(objective='reg:linear', n_estimators=50, max_depth=5, n_jobs=-1, random_state=42)\n",
    "xgr.fit(X_train.drop(columns=['haversine_dist']), y_train)\n",
    "\n",
    "y_pred = xgr.predict(X_test.drop(columns=['haversine_dist']))\n",
    "\n",
    "rsq_manhattan_xgb = r2_score(y_true=y_test, y_pred=y_pred)\n",
    "adj_rsq_manhattan_xgb = adjusted_r2_score(y_true=y_test, y_pred=y_pred, X_test=X_test)\n",
    "rmse_manhattan_xgb = mean_squared_error(y_true=y_test, y_pred=y_pred) ** 0.5\n",
    "print('R-sq:', rsq_manhattan_xgb)\n",
    "print('Adj. R-sq:', adj_rsq_manhattan_xgb)\n",
    "print('RMSE:', rmse_manhattan_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BRfDEKCcPR-v"
   },
   "source": [
    "Our model performance has actually not improved proving haversine distance was a superior feature as compared to manhattan distance here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QFRJ1y8PPR-w"
   },
   "source": [
    "### Dropping Manhattan Distance since it didn't yield a better model than Haversine Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yHrfKhmbPR-w"
   },
   "outputs": [],
   "source": [
    "X_train.drop(columns=['manhattan_dist'], inplace=True)\n",
    "X_test.drop(columns=['manhattan_dist'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w3fhNHCJPR-z"
   },
   "source": [
    "# Visualizing Pickup and Dropoff Trip Patterns\n",
    "\n",
    "The idea here is to visualize the pickup and dropoff coordinates of our taxi trips and see if there are any potential interesting patterns which can be used to engineer new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RdqwsNyFPR-z",
    "outputId": "c07950d2-475a-44eb-e8d7-991104648676"
   },
   "outputs": [],
   "source": [
    "nyc_lat = (40.477399, 40.917577)\n",
    "nyc_long = (-74.259090, -73.700272)\n",
    "import matplotlib.image as mpimg\n",
    "nyc_img = mpimg.imread('nyc_map.png')\n",
    "\n",
    "ax = df.plot(kind='scatter', x='pickup_longitude', y='pickup_latitude',\n",
    "        color='coral', s=.05, alpha=.6, figsize=(10,7))\n",
    "plt.imshow(nyc_img, extent=[nyc_long[0], nyc_long[1], nyc_lat[0], nyc_lat[1]], alpha=0.45)\n",
    "t = plt.title(\"Pickups Heatmap\")\n",
    "yl = plt.ylim(nyc_lat)\n",
    "xl = plt.xlim(nyc_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SMCKudwiPR-1",
    "outputId": "71599780-a7b8-462c-fd6c-6f6fced39a30"
   },
   "outputs": [],
   "source": [
    "ax = df.plot(kind='scatter', x='dropoff_longitude', y='dropoff_latitude',\n",
    "        color='limegreen', s=.05, alpha=.6, figsize=(10,7))\n",
    "plt.imshow(nyc_img, extent=[nyc_long[0], nyc_long[1], nyc_lat[0], nyc_lat[1]], alpha=0.45)\n",
    "t = plt.title(\"Dropoffs Heatmap\")\n",
    "yl = plt.ylim(nyc_lat)\n",
    "xl = plt.xlim(nyc_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VwC49yvDPR-4"
   },
   "source": [
    "An interesting pattern is that a lot of pickups seem to occur at airports and similar patterns with dropoffs. However a lot of dropoffs occur across NYC which is kind of expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nIDWqg5bPR-4"
   },
   "source": [
    "# Visualizing Airport Trips based on JFK Airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vRiZ2PYRPR-5",
    "outputId": "9a3cb48a-d9e1-4618-d3d6-6a28eaa7a6b6"
   },
   "outputs": [],
   "source": [
    "JFK = {\n",
    "    'min_lat':40.619,\n",
    "    'max_lat':40.665,\n",
    "    'min_lng':-73.835,\n",
    "    'max_lng':-73.740 \n",
    "}\n",
    "\n",
    "JFK_center=[40.642,-73.780]\n",
    "jfk_pickup_filter = ((df.pickup_latitude >= JFK['min_lat']) \n",
    "                      & (df.pickup_latitude <= JFK['max_lat']) \n",
    "                      & (df.pickup_longitude >= JFK['min_lng']) \n",
    "                      & (df.pickup_longitude <= JFK['max_lng']))\n",
    "jfk_dropoff_filter = ((df.dropoff_latitude >= JFK['min_lat']) \n",
    "                       & (df.dropoff_latitude <= JFK['max_lat']) \n",
    "                       & (df.dropoff_longitude >= JFK['min_lng']) \n",
    "                       & (df.dropoff_longitude <= JFK['max_lng']))\n",
    "\n",
    "jfk_pickups = df[jfk_pickup_filter]\n",
    "jfk_dropoffs = df[jfk_dropoff_filter]\n",
    "jfk_pickups.iloc[:450].to_csv('jfk_pickups.csv', index=False)\n",
    "jfk_dropoffs.iloc[:450].to_csv('jfk_dropoffs.csv', index=False)\n",
    "print('Total pickups from JFK:', jfk_pickups.shape[0])\n",
    "print('Total dropoffs to JFK:', jfk_dropoffs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6KnPfsQYPR-7"
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "JFK_center=[40.642,-73.780]\n",
    "m=folium.Map(location =JFK_center, zoom_start = 11,)\n",
    "\n",
    "for index,row in jfk_pickups.iloc[:450].iterrows():\n",
    "    pc = [row['pickup_latitude'], row['pickup_longitude']]\n",
    "    dc = [row['dropoff_latitude'], row['dropoff_longitude']]\n",
    "    folium.CircleMarker(pc,\n",
    "                        radius=1, color=\"red\", fill_opacity=0.9).add_to(m)\n",
    "    folium.CircleMarker(dc,\n",
    "                        radius=1, color=\"red\", fill_opacity=0.9).add_to(m)\n",
    "    \n",
    "    folium.PolyLine(locations=[pc, dc], color='red', smooth_factor=100, weight=0.75, opacity=0.6).add_to(m)\n",
    "\n",
    "for index,row in jfk_dropoffs.iloc[:450].iterrows():\n",
    "    pc = [row['pickup_latitude'], row['pickup_longitude']]\n",
    "    dc = [row['dropoff_latitude'], row['dropoff_longitude']]\n",
    "    folium.CircleMarker(pc,\n",
    "                        radius=1, color=\"limegreen\", fill_opacity=0.9).add_to(m)  \n",
    "    folium.CircleMarker(dc,\n",
    "                        radius=1, color=\"red\", fill_opacity=0.9).add_to(m)\n",
    "    folium.PolyLine(locations=[pc, dc], color='limegreen', smooth_factor=100, weight=0.65, opacity=0.25).add_to(m)\n",
    "    \n",
    "m.save('nyc_trips.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "02_50aUjPR-8"
   },
   "source": [
    "### View interactive map \n",
    "\n",
    "This gets reset for some reason when you reload the notebook so you might need to rerun the below cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xIOM366FPR-9"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, Image\n",
    "\n",
    "def embed_map(path=\"map.html\"):\n",
    "    \"\"\"\n",
    "    Embeds a linked iframe to the map into the IPython notebook.\n",
    "    \n",
    "    Note: this method will not capture the source of the map into the notebook.\n",
    "    This method should work for all maps (as long as they use relative urls).\n",
    "    \"\"\"\n",
    "    return display(\n",
    "              HTML('<iframe src=\"files/{path}\" style=\"width: 100%; height: 510px; border: none\"></iframe>'.format(path=path))\n",
    "    )\n",
    "\n",
    "# uncomment and run the below line to view the map\n",
    "#embed_map(path='nyc_trips.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gq5TONLVPR--"
   },
   "source": [
    "### View static map\n",
    "\n",
    "The same interactive map shown as a static map since it persists even after the notebook is closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k9oVo2YlPR-_",
    "outputId": "6ef464a4-3b92-4385-a147-205d5f6a9c59"
   },
   "outputs": [],
   "source": [
    "Image(data='jfk_trips_static.PNG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qGraM16JPR_B"
   },
   "source": [
    "### Visualizing price differences between airport and non-airport trips "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9-rp_SxEPR_D",
    "outputId": "d3d86419-cd61-4199-a6a0-5fc46f0e445e"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.kdeplot(np.log(jfk_pickups['fare_amount'].values),label='JFK Pickups', shade=True)\n",
    "sns.kdeplot(np.log(jfk_dropoffs['fare_amount'].values),label='JFK Dropoffs', shade=True, alpha=0.6)\n",
    "sns.kdeplot(np.log(df['fare_amount'].values),label='All Trips', shade=True)\n",
    "t = plt.title(\"Fare Amount Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Da_zOZcUPR_F"
   },
   "source": [
    "This gives us a good perspective of airport vs. non-airport trips and that we should definitely try and capture some of this using feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5NPiRbCpPR_G"
   },
   "source": [
    "# Experiment 6: Adding Airport Trips as Features\n",
    "\n",
    "It's time to add in airport specific trips as features. We will focus on the following key airports in NY\\NJ\n",
    "\n",
    "- Newark Liberty International Airport (EWR)\n",
    "- LaGuardia Airport (Laguardia)\n",
    "- John F. Kennedy International Airport (JFK)\n",
    "\n",
    "The idea is to create a bounding box for these airports and check if the trip pickup or dropoff co-ordinates are inside any of these airport bounding box co-ordinates.\n",
    "\n",
    "If the above holds good, it is a specific airport trip else not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rChsCj7LPR_G"
   },
   "outputs": [],
   "source": [
    "nyc_airports = {\n",
    "    'JFK':{\n",
    "        'min_lat':40.619,\n",
    "        'max_lat':40.665,\n",
    "        'min_lng':-73.835,\n",
    "        'max_lng':-73.740 \n",
    "    },\n",
    "              \n",
    "    'EWR':{\n",
    "        'min_lat':40.670, \n",
    "        'max_lat':40.709,\n",
    "        'min_lng':-74.193,\n",
    "        'max_lng':-74.149\n",
    "    },\n",
    "    \n",
    "    'LaGuardia':{\n",
    "        'min_lat':40.766, \n",
    "        'max_lat':40.786,\n",
    "        'min_lng':-73.889, \n",
    "        'max_lng':-73.855\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "def is_airport_trip(latitude, longitude, airport='JFK'):\n",
    "    \n",
    "    if (nyc_airports[airport]['min_lat'] <= latitude <= nyc_airports[airport]['max_lat'] \n",
    "       and nyc_airports[airport]['min_lng'] <= longitude<=nyc_airports[airport]['max_lng']):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dk5jfDh4PR_H"
   },
   "outputs": [],
   "source": [
    "#JFK based features\n",
    "X_train['is_pickup_jfk'] = X_train.apply(lambda row: is_airport_trip(latitude=row['pickup_latitude'], \n",
    "                                                                     longitude=row['pickup_longitude'],\n",
    "                                                                     airport='JFK'),\n",
    "                                         axis=1)\n",
    "X_train['is_dropoff_jfk'] = X_train.apply(lambda row: is_airport_trip(latitude=row['dropoff_latitude'],\n",
    "                                                                      longitude=row['dropoff_longitude'],\n",
    "                                                                      airport='JFK'),\n",
    "                                          axis=1)\n",
    "\n",
    "X_test['is_pickup_jfk'] = X_test.apply(lambda row: is_airport_trip(latitude=row['pickup_latitude'], \n",
    "                                                                     longitude=row['pickup_longitude'],\n",
    "                                                                     airport='JFK'),\n",
    "                                         axis=1)\n",
    "X_test['is_dropoff_jfk'] = X_test.apply(lambda row: is_airport_trip(latitude=row['dropoff_latitude'],\n",
    "                                                                      longitude=row['dropoff_longitude'],\n",
    "                                                                      airport='JFK'),\n",
    "                                          axis=1)\n",
    "\n",
    "\n",
    "# EWR based features\n",
    "X_train['is_pickup_ewr'] = X_train.apply(lambda row: is_airport_trip(latitude=row['pickup_latitude'], \n",
    "                                                                     longitude=row['pickup_longitude'],\n",
    "                                                                     airport='EWR'),\n",
    "                                         axis=1)\n",
    "X_train['is_dropoff_ewr'] = X_train.apply(lambda row: is_airport_trip(latitude=row['dropoff_latitude'],\n",
    "                                                                      longitude=row['dropoff_longitude'],\n",
    "                                                                      airport='EWR'),\n",
    "                                          axis=1)\n",
    "\n",
    "X_test['is_pickup_ewr'] = X_test.apply(lambda row: is_airport_trip(latitude=row['pickup_latitude'], \n",
    "                                                                     longitude=row['pickup_longitude'],\n",
    "                                                                     airport='EWR'),\n",
    "                                         axis=1)\n",
    "X_test['is_dropoff_ewr'] = X_test.apply(lambda row: is_airport_trip(latitude=row['dropoff_latitude'],\n",
    "                                                                      longitude=row['dropoff_longitude'],\n",
    "                                                                      airport='EWR'),\n",
    "                                          axis=1)\n",
    "\n",
    "\n",
    "# LaGuardia based features\n",
    "X_train['is_pickup_laguardia'] = X_train.apply(lambda row: is_airport_trip(latitude=row['pickup_latitude'], \n",
    "                                                                     longitude=row['pickup_longitude'],\n",
    "                                                                     airport='LaGuardia'),\n",
    "                                         axis=1)\n",
    "X_train['is_dropoff_laguardia'] = X_train.apply(lambda row: is_airport_trip(latitude=row['dropoff_latitude'],\n",
    "                                                                      longitude=row['dropoff_longitude'],\n",
    "                                                                      airport='LaGuardia'),\n",
    "                                          axis=1)\n",
    "\n",
    "X_test['is_pickup_laguardia'] = X_test.apply(lambda row: is_airport_trip(latitude=row['pickup_latitude'], \n",
    "                                                                     longitude=row['pickup_longitude'],\n",
    "                                                                     airport='LaGuardia'),\n",
    "                                         axis=1)\n",
    "X_test['is_dropoff_laguardia'] = X_test.apply(lambda row: is_airport_trip(latitude=row['dropoff_latitude'],\n",
    "                                                                      longitude=row['dropoff_longitude'],\n",
    "                                                                      airport='LaGuardia'),\n",
    "                                          axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2sRECY1DPR_J",
    "outputId": "7fc6e9f7-d86e-4cf1-8ee3-e1adb656e336"
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2A5XtPmQPR_K",
    "outputId": "1314e6a8-9320-4a41-f05b-efbd425dd6e1"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "xgr = xgb.XGBRegressor(objective='reg:linear', n_estimators=50, max_depth=5, n_jobs=-1, random_state=42)\n",
    "xgr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgr.predict(X_test)\n",
    "\n",
    "rsq_airports_xgb = r2_score(y_true=y_test, y_pred=y_pred)\n",
    "adj_rsq_airports_xgb = adjusted_r2_score(y_true=y_test, y_pred=y_pred, X_test=X_test)\n",
    "rmse_airports_xgb = mean_squared_error(y_true=y_test, y_pred=y_pred) ** 0.5\n",
    "print('R-sq:', rsq_airports_xgb)\n",
    "print('Adj. R-sq:', adj_rsq_airports_xgb)\n",
    "print('RMSE:', rmse_airports_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "20d6TKN2PR_L"
   },
   "source": [
    "We have __R-sq: 0.831__ and __RMSE: 3.989__ in our new model. \n",
    "\n",
    "This is an improvement of __0.1% R-sq__ and __0.008__ drop in RMSE.\n",
    "\n",
    "A minuscule improvement, but even point helps. Is it possible to improve further?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QNHQhpL_PR_M"
   },
   "source": [
    "# Visualizing patterns between Fare Amount and Trip Distance\n",
    "\n",
    "We want to see if trip fare consistently increases with distance or are there any other patterns.\n",
    "\n",
    "Also are these patterns the same for airport and non-airport trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wtCbmrGSPR_N",
    "outputId": "cf97f0c2-e2d5-4c21-e111-b9ae0a1b6f07"
   },
   "outputs": [],
   "source": [
    "airport_trip_filter = ((X_train.is_pickup_jfk == 1) | (X_train.is_dropoff_jfk == 1) \n",
    "                       | (X_train.is_pickup_ewr == 1) | (X_train.is_dropoff_ewr == 1)\n",
    "                       | (X_train.is_pickup_laguardia == 1) | (X_train.is_dropoff_laguardia == 1))\n",
    "\n",
    "airport_trips = X_train[airport_trip_filter]\n",
    "nonairport_trips = X_train[~airport_trip_filter]\n",
    "airport_trips.shape, nonairport_trips.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LAyHOaU1PR_Q",
    "outputId": "6b5c9508-33e1-462a-b903-b13b16bd0324"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (14, 5))\n",
    "\n",
    "ax1 = fig.add_subplot(1,2, 1)\n",
    "sp = ax1.scatter(x=airport_trips['haversine_dist'],y=y_train[airport_trips.index], color='limegreen')\n",
    "xl = ax1.set_xlabel(\"Trip Distance\")\n",
    "yl = ax1.set_ylabel(\"Fare Amount\")\n",
    "t = ax1.set_title(\"Trip Distance vs Fare Amount - Airport Trips\")\n",
    "\n",
    "ax2 = fig.add_subplot(1,2, 2)\n",
    "sp = ax2.scatter(x=airport_trips['haversine_dist'],y=y_train[airport_trips.index], color='limegreen')\n",
    "ax2.set_xlim([0, 200])\n",
    "xl = ax2.set_xlabel(\"Trip Distance\")\n",
    "yl = ax2.set_ylabel(\"Fare Amount\")\n",
    "t = ax2.set_title(\"Trip Distance vs Fare Amount (dist < 200) - Airport Trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JyIXO74JPR_T",
    "outputId": "d188a51c-75c2-4591-f619-d6e6e7438615"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (14, 5))\n",
    "\n",
    "ax1 = fig.add_subplot(1,2, 1)\n",
    "sp = ax1.scatter(x=nonairport_trips['haversine_dist'],y=y_train[nonairport_trips.index], color='coral')\n",
    "xl = ax1.set_xlabel(\"Trip Distance\")\n",
    "yl = ax1.set_ylabel(\"Fare Amount\")\n",
    "t = ax1.set_title(\"Trip Distance vs Fare Amount - Non-Airport Trips\")\n",
    "\n",
    "ax2 = fig.add_subplot(1,2, 2)\n",
    "sp = ax2.scatter(x=nonairport_trips['haversine_dist'],y=y_train[nonairport_trips.index], color='coral')\n",
    "ax2.set_xlim([0, 200])\n",
    "xl = ax2.set_xlabel(\"Trip Distance\")\n",
    "yl = ax2.set_ylabel(\"Fare Amount\")\n",
    "t = ax2.set_title(\"Trip Distance vs Fare Amount (dist < 200) - Non-Airport Trips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6yQlGTN-PR_V"
   },
   "source": [
    "Looks like regular trips have a distance of < 80 for airport trips and < 60 for non-airport trips where fare has a strong positive correlation with distance.\n",
    "\n",
    "For trips greater than this distance, looks like they might have a fixed fee (or could be potential outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6w8U6-1DPR_V"
   },
   "source": [
    "# Experiment 7: Experimenting with Long and Short Trips as a Feature\n",
    "\n",
    "Based on the previous analysis, we engineer a new feature to identify if a trip is a regular or a long trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KFPYJvbLPR_W"
   },
   "outputs": [],
   "source": [
    "def is_long_trip(record):\n",
    "    if ((record.is_pickup_jfk == 1) | (record.is_dropoff_jfk == 1) \n",
    "         | (record.is_pickup_ewr == 1) | (record.is_dropoff_ewr == 1)\n",
    "         | (record.is_pickup_laguardia == 1) | (record.is_dropoff_laguardia == 1)):\n",
    "        airport_trip = True\n",
    "    else:\n",
    "        airport_trip = False\n",
    "        \n",
    "    if is_airport_trip:\n",
    "        if record.haversine_dist >= 80:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        if record.haversine_dist >= 60:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7x2Qi3sAPR_X"
   },
   "outputs": [],
   "source": [
    "X_train['is_long_trip'] = X_train.apply(lambda row: is_long_trip(row), axis=1)\n",
    "X_test['is_long_trip'] = X_test.apply(lambda row: is_long_trip(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r-SOPFYwPR_Z",
    "outputId": "7d61b330-bdc3-4c02-b661-2c81f3e8dc7c"
   },
   "outputs": [],
   "source": [
    "X_test['is_long_trip'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "phx8ahWQPR_b",
    "outputId": "f372a979-e3c2-48d0-d31a-8712a86cee1d"
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d095lBAYPR_d",
    "outputId": "0d968965-0752-48b4-cb62-aea686b9aece"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "xgr = xgb.XGBRegressor(objective='reg:linear', n_estimators=50, max_depth=5, n_jobs=-1, random_state=42)\n",
    "xgr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgr.predict(X_test)\n",
    "\n",
    "rsq_trip_len_xgb = r2_score(y_true=y_test, y_pred=y_pred)\n",
    "adj_rsq_trip_len_xgb = adjusted_r2_score(y_true=y_test, y_pred=y_pred, X_test=X_test)\n",
    "rmse_trip_len_xgb = mean_squared_error(y_true=y_test, y_pred=y_pred) ** 0.5\n",
    "print('R-sq:', rsq_trip_len_xgb)\n",
    "print('Adj. R-sq:', adj_rsq_trip_len_xgb)\n",
    "print('RMSE:', rmse_trip_len_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WNz9846ePR_f"
   },
   "source": [
    "There is no marked improvement in the model performance and this is expected considering the number of long trips are far too less as compared to rest of the data points. \n",
    "\n",
    "Also several of them could be potential outliers considering they may not even be trips inside NYC and we might consider removing them in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kw0iQfmiPR_f"
   },
   "source": [
    "# Experiment 8: Adding Borough based trips as features\n",
    "\n",
    "![](nyc_boroughs.PNG)\n",
    "\n",
    "There are a total of five boroughs in New York City\n",
    "\n",
    "The term borough was adopted to describe a form of governmental administration for each of the five fundamental constituent parts of NYC\n",
    "\n",
    "The idea is that trips to and from different boroughs might have different pricing patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0jIfJgYQPR_f"
   },
   "outputs": [],
   "source": [
    "nyc_boroughs={\n",
    "    'manhattan':{\n",
    "        'min_lng':-74.047,\n",
    "        'min_lat':40.682,\n",
    "        'max_lng':-73.906,\n",
    "        'max_lat':40.882\n",
    "    },\n",
    "    \n",
    "    'queens':{\n",
    "        'min_lng':-73.963,\n",
    "        'min_lat':40.543,\n",
    "        'max_lng':-73.700,\n",
    "        'max_lat':40.800\n",
    "\n",
    "    },\n",
    "\n",
    "    'brooklyn':{\n",
    "        'min_lng':-74.042,\n",
    "        'min_lat':40.570,\n",
    "        'max_lng':-73.833,\n",
    "        'max_lat':40.739\n",
    "\n",
    "    },\n",
    "\n",
    "    'bronx':{\n",
    "        'min_lng':-73.933,\n",
    "        'min_lat':40.785,\n",
    "        'max_lng':-73.765,\n",
    "        'max_lat':40.917\n",
    "\n",
    "    },\n",
    "\n",
    "    'staten_island':{\n",
    "        'min_lng':-74.255,\n",
    "        'min_lat':40.496,\n",
    "        'max_lng':-74.052,\n",
    "        'max_lat':40.649\n",
    "\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "def get_borough_name(lat,lng):\n",
    "    \n",
    "    locs=nyc_boroughs.keys()\n",
    "    for loc in locs:\n",
    "        if (lat>=nyc_boroughs[loc]['min_lat'] \n",
    "           and lat<=nyc_boroughs[loc]['max_lat'] \n",
    "           and lng>=nyc_boroughs[loc]['min_lng'] \n",
    "           and lng<=nyc_boroughs[loc]['max_lng']):\n",
    "            return loc\n",
    "    return 'others'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tnnQ7CciPR_i"
   },
   "outputs": [],
   "source": [
    "X_train['pickup_borough'] = X_train.apply(lambda row: get_borough_name(row['pickup_latitude'],row['pickup_longitude']),axis=1)\n",
    "X_train['dropoff_borough'] = X_train.apply(lambda row: get_borough_name(row['dropoff_latitude'],row['dropoff_longitude']),axis=1)\n",
    "\n",
    "X_test['pickup_borough'] = X_test.apply(lambda row: get_borough_name(row['pickup_latitude'],row['pickup_longitude']),axis=1)\n",
    "X_test['dropoff_borough'] = X_test.apply(lambda row: get_borough_name(row['dropoff_latitude'],row['dropoff_longitude']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gMmxEc5VPR_j",
    "outputId": "d47191a3-d361-4c06-fb7a-2ba58a96342f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7R7zFMQEPR_k"
   },
   "source": [
    "Based on these categorical features we obtained, we can leverage label encoding or one-hot encoding and build our models. Let's try both!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pBHPfpn3PR_l"
   },
   "source": [
    "# Experiment 8A: Label-Encoding Borough Trip Features\n",
    "\n",
    "Tree-based models natively can actually handle categorical variables without the need to often explicitely one-hot encode them as opposed to other models (especially parametric ones).\n",
    "\n",
    "Let's engineer these features as label encoded features and build our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8oVM4BmmPR_n"
   },
   "outputs": [],
   "source": [
    "X_train_le = X_train.copy(deep=True)\n",
    "X_train_le['pickup_borough'] = X_train['pickup_borough'].astype('category').cat.codes\n",
    "X_train_le['dropoff_borough'] = X_train['dropoff_borough'].astype('category').cat.codes\n",
    "\n",
    "X_test_le = X_test.copy(deep=True)\n",
    "X_test_le['pickup_borough'] = X_test['pickup_borough'].astype('category').cat.codes\n",
    "X_test_le['dropoff_borough'] = X_test['dropoff_borough'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ni8EMsUCPR_q",
    "outputId": "3706ab2f-f2b0-4542-f1e3-0931ab26c124",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_le.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RWSlHISaPR_s",
    "outputId": "2dde55c9-0a28-40b7-e46d-a03fc00845ba"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "xgr = xgb.XGBRegressor(objective='reg:linear', n_estimators=50, max_depth=5, n_jobs=-1, random_state=42)\n",
    "xgr.fit(X_train_le, y_train)\n",
    "\n",
    "y_pred = xgr.predict(X_test_le)\n",
    "\n",
    "rsq_borough_xgb = r2_score(y_true=y_test, y_pred=y_pred)\n",
    "adj_rsq_borough_xgb = adjusted_r2_score(y_true=y_test, y_pred=y_pred, X_test=X_test_le)\n",
    "rmse_borough_xgb = mean_squared_error(y_true=y_test, y_pred=y_pred) ** 0.5\n",
    "print('R-sq:', rsq_borough_xgb)\n",
    "print('Adj. R-sq:', adj_rsq_borough_xgb)\n",
    "print('RMSE:', rmse_borough_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0infXyYiPR_v"
   },
   "source": [
    "Looks like these features are not having any effect on our model! This could be because there are other more dominant features which are selected as important features given that we are using a tree-based ensemble model following boosting priciples. \n",
    "\n",
    "Also the max-depth of each tree is 5 which can be a reason for these features not getting selected at all!\n",
    "\n",
    "Let's try one-hot encoding now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wiYRTGcMPR_w"
   },
   "source": [
    "# Experiment 8B: One-hot Encoding Borough Trip Features\n",
    "\n",
    "Now it is time to apply one-hot encoding to our borough features. In one-hot encoding, if you have __n__ different categories in a categorical feature, you end up with __n__ columns depicting each distinct category as a binary feature (present or absent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9f0SC-xPR_y"
   },
   "outputs": [],
   "source": [
    "X_train_ohe = pd.get_dummies(X_train)\n",
    "X_test_ohe = pd.get_dummies(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lrpz2fBePR_1",
    "outputId": "b448916e-3f9f-4c76-ac73-cb4f78d96b84"
   },
   "outputs": [],
   "source": [
    "X_train_ohe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D6NYz3KwPR_5",
    "outputId": "d7dd64ed-7001-4b85-dd11-a9eea1c89618"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "xgr = xgb.XGBRegressor(objective='reg:linear', n_estimators=50, max_depth=5, n_jobs=-1, random_state=42)\n",
    "xgr.fit(X_train_ohe, y_train)\n",
    "\n",
    "y_pred = xgr.predict(X_test_ohe)\n",
    "\n",
    "rsq_borough_xgb = r2_score(y_true=y_test, y_pred=y_pred)\n",
    "adj_rsq_borough_xgb = adjusted_r2_score(y_true=y_test, y_pred=y_pred, X_test=X_test_ohe)\n",
    "rmse_borough_xgb = mean_squared_error(y_true=y_test, y_pred=y_pred) ** 0.5\n",
    "print('R-sq:', rsq_borough_xgb)\n",
    "print('Adj. R-sq:', adj_rsq_borough_xgb)\n",
    "print('RMSE:', rmse_borough_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1-rVuobgPR_9"
   },
   "source": [
    "We have __R-sq: 0.832__ and __RMSE: 3.976__ in our new model. \n",
    "\n",
    "This is an improvement of __0.1% R-sq__ and __0.013__ drop in RMSE.\n",
    "\n",
    "A minuscule improvement. Maybe increasing the depth of the trees and more trees might help in this aspect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jlGMvxCuPR_-"
   },
   "source": [
    "# Training a full-fledged XGBoost Model on the final featureset\n",
    "\n",
    "The intent here is to not really tune the model but see if more trees with a larger max depth help in improving the model.\n",
    "\n",
    "We recommend leveraging cross-validation always for tuning and to __never__ ever tune on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S1o3Z27uPR_-",
    "outputId": "eefc084a-b79b-4768-d157-b040b6b3a6e7"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "xgr = xgb.XGBRegressor(objective='reg:linear', n_estimators=1000, max_depth=10, n_jobs=-1, random_state=42)\n",
    "xgr.fit(X_train_ohe, y_train)\n",
    "\n",
    "y_pred = xgr.predict(X_test_ohe)\n",
    "\n",
    "rsq_final_xgb = r2_score(y_true=y_test, y_pred=y_pred)\n",
    "adj_rsq_final_xgb = adjusted_r2_score(y_true=y_test, y_pred=y_pred, X_test=X_test_ohe)\n",
    "rmse_final_xgb = mean_squared_error(y_true=y_test, y_pred=y_pred) ** 0.5\n",
    "print('R-sq:', rsq_final_xgb)\n",
    "print('Adj. R-sq:', adj_rsq_final_xgb)\n",
    "print('RMSE:', rmse_final_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iVBAaLLhPSAD"
   },
   "source": [
    "A nice jump in the __R-sq__ by __3.3%__ to give us a final score of __86.5%__\n",
    "\n",
    "__RMSE__ also drops by __0.424__ to give a final score of __3.552__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "64vVQbA9PSAD"
   },
   "source": [
    "# Future Scope\n",
    "\n",
    "Try experimenting with time based features like taking into account seasonality, drill down into specific boroughs for more patterns with regard to trips, distances. Maybe even tourist points like Statue of Liberty? Focus only on NYC based data points instead of complete US. Add in more data points to train!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "EUTLObThPR-H",
    "64vVQbA9PSAD"
   ],
   "name": "Case Study 1 - NYC Taxi Fare Prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
